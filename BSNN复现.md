# BSNNå¤ç°

### è®ºæ–‡ç†è§£

æœ¬è´¨å…¶å®æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œè§£å†³çš„é—®é¢˜æ˜¯å¯¹ç½‘ç»œåè®®åˆ†ç±»ï¼Œä½†ä¸åŒäºä»¥å¾€çš„åè®®åˆ†ç±»é—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ä½¿ç”¨æŠ¥æ–‡å¤´ç­‰ç‰¹å¾ä¿¡æ¯ï¼Œè€Œæ˜¯çº¯ç²¹åœ°ä»payloadç‰¹å¾æå–è§’åº¦å¯¹ç½‘ç»œåè®®åˆ†ç±»ã€‚å› æ­¤å¯ä»¥çœ‹ä½œä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œè¾“å…¥ä¸€ä¸ªpacketçš„payloadéƒ¨åˆ†ï¼Œè¾“å‡ºæŠ¥æ–‡åè®®ç±»å‹ã€‚å…¶ä¼˜ç‚¹åœ¨äºï¼š

- å¯ä»¥å­¦ä¹ æ–°çš„åè®®ï¼Œè€Œä¸è¢«é™åˆ¶äºå·²æœ‰åè®®
- ä¸éœ€è¦ç¹å¤çš„å¯¹æŠ¥æ–‡å¤´ç‰¹å¾ä¿¡æ¯çš„æŒ–æ˜ï¼ˆå¹¶ä¸”è¿™äº›ä¿¡æ¯å¯èƒ½æ˜¯ä¸é è°±çš„ï¼Œå¯ä»¥ä¿®æ”¹çš„è¯ï¼‰

<img src="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/1.png" width = "450" height = "250" align=center />

æ¨¡å‹ç»“æ„å¦‚ä¸Šæ‰€ç¤ºï¼Œæ˜¯ä¸€ä¸ªHierarchical attention networkã€‚

å…ˆè§£é‡Šä¸€ä¸‹æ¨¡å‹ä¸­çš„å„ä¸ªå­—æ®µçš„æ„æ€ã€‚DatagramæŒ‡çš„æ˜¯ä¸€ä¸ªpacketä¸­çš„payloadéƒ¨åˆ†ï¼Œå¦‚ä¸‹å›¾wiresharkæŠ“åŒ…åè“è‰²çš„byteéƒ¨åˆ†ã€‚è¿›ä¸€æ­¥å¯¹Datagramåšåˆ’åˆ†å¯ä»¥å¾—åˆ°å¾ˆå¤šä¸ªé•¿åº¦å‡ç­‰çš„Segmentï¼Œè¿™é‡Œå¦‚æœè®¾å®šæ¯ä¸ªsegmenté•¿åº¦éƒ½ä¸º5çš„è¯ï¼Œä¸€ä¸ªsegmentå°±å¯ä»¥è¡¨è¾¾ä¸ºä¸‹å›¾ä¸­çº¢è‰²æ–¹æ¡†éƒ¨åˆ†ã€‚å¯ä»¥çœ‹åˆ°ä¸€ä¸ªsegment[3b, de, 01, 00, 00]é‡Œæœ‰å¤šä¸ªåå…­è¿›åˆ¶å­—ç¬¦ã€‚

è¿™é‡Œä¸ºäº†æ›´å¥½çš„ç†è§£ï¼Œå¯ä»¥åšä¸€ä¸ªç±»æ¯”ï¼š

- ä¸€ç¯‡æ–‡ç«  ==>  datagram ==> [3b, de, 01, 00, 00, 01, 00, 00......]
- æ–‡ç« ä¸­çš„å¥å­ ==> segment ==> [3b, de, 01, 00, 00]
- å¥å­ä¸­çš„å•è¯ ==> 3b

<img src="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/2.png" width = "450" height = "250" align=center />

ç°åœ¨è¦å¯¹è¿™æ ·ä¸€ä¸ªç»“æ„çš„datagramåšè®­ç»ƒï¼Œæœ€åéœ€è¦å­¦åˆ°å®ƒæ‰€å±çš„åè®®ç±»å‹ã€‚æ¨¡å‹çš„ç›´è§‰å°±æ˜¯æˆ‘ä»¬é¦–å…ˆå»å…³æ³¨ä¸€ä¸ªå¥å­ï¼ˆsegmentï¼‰ä¸­å•è¯ï¼ˆcharacterï¼‰çš„è¡¨è¾¾ï¼Œä½¿ç”¨rnn(LSTM/GRU)è®­ç»ƒå¹¶ä¸”æ¯ä¸ªå•è¯å¯¹äºè¿™ä¸ªå¥å­çš„é‡è¦æ€§æ˜¯ä¸åŒçš„ï¼ˆattentionæœºåˆ¶ï¼‰ï¼Œé€šè¿‡è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹å¾—åˆ°è¿™ä¸ªå¥å­ï¼ˆsegmentï¼‰çš„è¡¨è¾¾ã€‚ä¹‹åå†é‡å¤ç±»ä¼¼çš„è¿‡ç¨‹ï¼Œåœ¨ä¸€ç¯‡æ–‡ç« ï¼ˆdatagramï¼‰ä¸­ï¼Œä¸åŒå¥å­çš„é‡è¦æ€§ä¸åŒï¼Œå…³æ³¨äºâ€œç„¦ç‚¹â€ï¼Œé€šè¿‡ä¸€ä¸ªattention encoderå¾—åˆ°è¿™ç¯‡æ–‡ç« ï¼ˆdatagramï¼‰çš„è¡¨è¾¾ï¼Œä½¿ç”¨softmaxç­‰æœ€åå¾—åˆ°è¿™ä¸ªæ–‡ç« ï¼ˆdatagramï¼‰çš„ç±»åˆ«ã€‚

æ•´ä¸ªæ¨¡å‹æœ‰ä¸¤å±‚attention encoderï¼Œä½¿ç”¨çš„RNN Unitæ˜¯LSTM/GRUï¼ˆæˆ‘åé¢å®ç°çš„GRUï¼Œå› ä¸ºå¯¹æˆ‘çš„ç”µè„‘å‹å¥½ä¸€äº›ğŸ¶ï¼‰ï¼Œä¸”æ˜¯bidirectionalçš„ï¼ˆåŒå‘ï¼‰ï¼Œå› ä¸ºè¿™é‡Œä¸Šä¸‹æ–‡éƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œéœ€è¦å…¨å±€çš„ã€‚å®ç°ç»†èŠ‚ä¸Šä½¿ç”¨Focal Lossï¼Œä¸»è¦æ˜¯ä¸ºäº†å¤„ç†æ ·æœ¬æ•°æ®ä¸å‡è¡¡çš„é—®é¢˜ï¼ˆå¥½å·§ä¸å·§æˆ‘åšæ•°æ®çš„æ—¶å€™æ­£å¥½æ¯ä¸ªç±»åˆ«éƒ½æ˜¯å‡åˆ†çš„ï¼Œä¼¼ä¹å¯¹æˆ‘è¿™ä¸ªæ²¡æœ‰å¤šå¤§ç”¨å¤„ï¼Œæˆ‘çš„é”™ï¼‰ï¼Œè¯„ä¼°æŒ‡æ ‡ä½¿ç”¨$F_1$ã€‚

### ä»£ç å¤ç°

ä»£ç ç›®å½•ä¸­dataæ–‡ä»¶å¤¹å†…ä¸ºæ•°æ®ï¼Œtryæ–‡ä»¶å¤¹ä¸­ä¸ºç¬¬ä¸€æ¬¡å°è¯•çš„pytorchä»£ç ï¼ˆnnç»“æ„æœ‰é—®é¢˜ï¼Œå½“æ—¶ä¸å¤ªææ˜ç™½å¤šå±‚attentionæ€ä¹ˆå†™ï¼Œä½†ä¹Ÿæ˜¯ä¸ªå®è·µè¿‡ç¨‹å…ˆä¿å­˜ä¸‹æ¥ï¼‰ï¼Œrun_BSNNä¿å­˜çš„è¿è¡Œæ•°æ®ï¼Œpre.pyåšæ•°æ®é¢„å¤„ç†ï¼ŒDataUtil.pyåšæ•°æ®paddingåŠåˆ’åˆ†ï¼ŒBSNN.pyæ”¾æ¨¡å‹ï¼Œtrain.pyä¸ºè¿è¡Œå…¥å£ã€‚

tensorflowç‰ˆæœ¬ä¸º1.14ï¼ˆä¸»è¦æƒ³ä½¿ç”¨ä¸€ä¸ªè€ç‰ˆæœ¬çš„åº“ï¼Œé™åˆ°è¿™ä¸ªç‰ˆæœ¬ï¼Œwarningè¿˜æ˜¯æœ‰çš„ğŸ˜…ï¼‰ï¼Œpreä½¿ç”¨çš„pytorchï¼ˆä½†æ˜¯ç›¸å…³æ•°æ®é¢„å¤„ç†å·²è¿è¡Œå‡ºæ¥ä¿å­˜å¥½äº†ï¼Œå› æ­¤ä¸éœ€è¦å†ä½¿ç”¨pytorchè¿è¡Œpre.pyï¼‰

#### æ•°æ®é¢„å¤„ç†

è®ºæ–‡ä¸­æ•°æ®é›†æ˜¯è‡ªå·±æ”¶é›†äº†ï¼Œå¹¶ä¸”åœ¨ç½‘ä¸Šæ‰¾äº†ä¸€ä¸‹ç°å­˜æ•°æ®é›†ï¼Œæ¯”è¾ƒéš¾å¾—åˆ°æ¡ä»¶é€‚åˆçš„ï¼Œå› æ­¤è‡ªå·±æµé‡æŠ“åŒ…æ”¶é›†äº†300ç¬”æ•°æ®ï¼ˆå¤šäº†æˆ‘ç”µè„‘ä¼¤ä¸èµ·æ¬¸ğŸ˜­ï¼‰ã€‚æ•°æ®ç‰¹å¾å¦‚ä¸‹ï¼š

- æŠ“çš„æ˜¯DNSï¼ŒOICQï¼ŒQUICä¸‰ä¸ªåè®®ï¼Œæ¯ä¸ªæŠ“å–äº†100ç¬”ï¼Œå…¶ä¸­åä¸¤ä¸ªæ˜¯googleä½¿ç”¨çš„å¿«ä¼ åè®®å’ŒQQä½¿ç”¨çš„åè®®
- ä¸€äº›æŠ¥æ–‡å†…å®¹è¾ƒçŸ­ï¼Œä¸€äº›å¾ˆé•¿ï¼Œè¿™ä¸ªç‰¹ç‚¹ä½¿å¾—åšæ•°æ®paddingæ—¶è¾ƒéš¾é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ç»Ÿä¸€é•¿åº¦ï¼ˆæˆ‘è¿™é‡Œæœ€åé€‰æ‹©çš„100ä½œä¸ºå›ºå®šé•¿ï¼‰
- å› ä¸ºæ˜¯è¿ç»­åœ°æŠ“å–ï¼Œå‰åæŠ¥æ–‡ç›¸ä¼¼åº¦å¾ˆå¤šï¼Œå› æ­¤æ¯ç±»åè®®å¤šæ ·æ€§ä¸è¶³

é¢„å¤„ç†æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<img src="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/3.png" width = "450" height = "250" align=center />

pre.pyä¸­æ•°æ®å¤„ç†ä»£ç 

```python
# -*- coding: utf-8 -*-
import json
import csv
import pandas as pd
import numpy as np

# ä¸€ä¸ªsegmentçš„é•¿åº¦ï¼Œè®ºæ–‡ä¸­æ¨èçš„æ˜¯L=8
SEGMENT_LEN = 8

packets = []

# ä¸‹é¢ä¸‰ä¸ªéƒ½æ˜¯å¯¹æŠ¥æ–‡æ•°æ®åšå¤„ç†ï¼Œæå–å‡ºéœ€è¦çš„payloadéƒ¨åˆ†ï¼Œå­˜å‚¨ä¸‹æ¥
with open('data/QUIC.json', 'r', encoding='UTF-8') as reader:
    content = json.load(reader)
    #len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†
    for i in range(0, 100):
        packet = ['0', content[i]["_source"]["layers"]["udp"]["udp.payload"]]
        packets.append(packet)
    reader.close()

# QQçš„åè®®
with open('data/OICQ.json', 'r', encoding='UTF-8') as reader:
    content = json.load(reader)
    #len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†
    for i in range(0, 100):
        packet = ['1', content[i]["_source"]["layers"]["udp"]["udp.payload"]]
        packets.append(packet)
    reader.close()

with open('data/DNS.json', 'r', encoding='UTF-8') as reader:
    content = json.load(reader)
    #len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†
    for i in range(0, 100):
        packet = ['2', content[i]["_source"]["layers"]["udp"]["udp.payload"]]
        packets.append(packet)
    reader.close()

# trainæ•°æ®å†™å…¥csv ==> payload.csv
csv_fp = open("data/train.csv", "w", encoding='utf-8', newline='')
writer = csv.writer(csv_fp)
writer.writerow(['label', 'payload'])
writer.writerows(packets)
csv_fp.close()

"""
å°†payloadä¸­å­—ç¬¦è½¬åŒ–ä¸º0-255æ•°å­—
å°†æ¯ä¸ªdatagramçš„payloadæŒ‰ç…§é•¿åº¦N=SEGMENT_LENåˆ’åˆ†
return: [[label, [[0,1,2,3,...][]....]]]
"""
def segment_slice():
    data = pd.read_csv('data/train.csv', encoding='UTF-8')
    labels = data['label']
    payloads = data['payload'].apply(lambda x :x.split(':'))
    Datagrams = [] # [[label, [0,255,...]],...]
    for i in range(0, len(data)):
        Datagram = [] # [label, [0,255,...]]
        for j in range(0, len(payloads[i])):
            Datagram.append(int(payloads[i][j], 16))
        Datagrams.append([labels[i], Datagram])

    # åˆ‡ç‰‡
    for i in range(0, len(Datagrams)):
        last_num = len(Datagrams[i][1]) % SEGMENT_LEN
        Datagrams[i][1] = Datagrams[i][1] + [0]*(SEGMENT_LEN-last_num)
        temp = np.array(Datagrams[i][1])
        temp = temp.reshape(-1, SEGMENT_LEN)
        Datagrams[i][1] = temp

    return Datagrams
```

DataUtil.pyä¸­è¿›ä¸€æ­¥å¤„ç†

```python
import numpy as np
import pre as pre
import random

"""
datagramçš„segmentæ•°é‡ä¸ä¸€è‡´ï¼Œéƒ½ç»Ÿä¸€ä¸ºmaxlenå¤§å° ç©ºçš„å¡«å……0å‘é‡
ä¸è¶³ï¼š0çš„å¡«å……æ¯”è¾ƒå¤š
"""
def pad_data_x(data_xs, maxlen=100, PAD=0):  
    padded_data_xs = []
    for data_x in data_xs:
        # ä¸€ä¸ªdatagramçš„
        if len(data_x) >= maxlen:
            padded_data_x = data_x[:maxlen]
        else:
            padded_data_x = data_x
            zero_len = maxlen-len(padded_data_x)
            zero = np.zeros([zero_len,8], int)
            padded_data_x = np.insert(padded_data_x, len(data_x), values=zero, axis=0)
            
        padded_data_xs.append(padded_data_x)
         
    return padded_data_xs

def read_dataset():
    num_classes = 3
    datagrams = pre.segment_slice()

    random.shuffle(datagrams) # æ‰“ä¹±æ•°æ®é›†

    data_x = []
    data_y = []
    for datagram in datagrams:
        label = datagram[0]
        labels = [0] * num_classes
        labels[label-1] = 1 # è½¬åŒ–ä¸ºone-hot
        data_y.append(labels)
        data_x.append(datagram[1])
    data_x = pad_data_x(data_x)

    # åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†
    length = len(data_x)
    train_x, dev_x = data_x[:int(length*0.9)], data_x[int(length*0.9)+1 :]
    train_y, dev_y = data_y[:int(length*0.9)], data_y[int(length*0.9)+1 :]
    return train_x, train_y, dev_x, dev_y
```

#### æ¨¡å‹æ„å»º

æ¨¡å‹ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼š

- word encoder ï¼ˆBiGRU layerï¼‰
- word attention ï¼ˆAttention layerï¼‰
- sentence encoder ï¼ˆBiGRU layerï¼‰
- sentence attention ï¼ˆAttention layerï¼‰

##### GRUåŸç†

<img src="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/4.png" width = "450" height = "250" align=center />

GRUæ˜¯RNNçš„ä¸€ä¸ªå˜ç§ï¼Œä½¿ç”¨é—¨æœºåˆ¶æ¥è®°å½•å½“å‰åºåˆ—çš„çŠ¶æ€ã€‚åœ¨GRUä¸­æœ‰ä¸¤ç§ç±»å‹çš„é—¨ï¼ˆgateï¼‰: reset gateå’Œupdate gateã€‚è¿™ä¸¤ä¸ªé—¨ä¸€èµ·æ§åˆ¶æ¥å†³å®šå½“å‰çŠ¶æ€æœ‰å¤šå°‘ä¿¡æ¯è¦æ›´æ–°ã€‚

reset gateæ˜¯ç”¨äºå†³å®šå¤šå°‘è¿‡å»çš„ä¿¡æ¯è¢«ç”¨äºç”Ÿæˆå€™é€‰çŠ¶æ€ï¼Œå¦‚æœRtä¸º0ï¼Œè¡¨æ˜å¿˜è®°ä¹‹å‰çš„æ‰€æœ‰çŠ¶æ€ï¼š
$$
r_t = \sigma(W_rx_r+U_rh_{y-1}+b_r)
$$
æ ¹æ®reset gateçš„åˆ†æ•°å¯ä»¥è®¡ç®—å‡ºå€™é€‰çŠ¶æ€ï¼š
$$
\hat{h_t} = tanh(W_hx_t+r_t\oplus(U_hh_{t-1}+b_h))
$$
update gateæ˜¯ç”¨æ¥å†³å®šç”±å¤šå°‘è¿‡å»çš„ä¿¡æ¯è¢«ä¿ç•™ï¼Œä»¥åŠå¤šå°‘æ–°ä¿¡æ¯è¢«åŠ è¿›æ¥ï¼š
$$
z_t = \sigma(W_zx_r+U_zh_{t-1}+b_z)
$$
æœ€åï¼Œéšè—å±‚çŠ¶æ€çš„è®¡ç®—å…¬å¼ï¼Œæœ‰update gateã€å€™é€‰çŠ¶æ€å’Œä¹‹å‰çš„çŠ¶æ€å…±åŒå†³å®šï¼š
$$
h_t = (1-z_t)\oplus h_{t-1}+z_t \oplus \hat{h_t}
$$

##### AttentionåŸç†

<img src="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/5.png" width = "450" height = "250" align=center />

##### 1ã€word encoder layer

é¦–å…ˆï¼Œå°†æ¯ä¸ªsegmentä¸­çš„characteråšembeddingè½¬æ¢æˆå‘é‡ï¼ˆè¿™é‡Œä½¿ç”¨one-hotï¼‰ï¼Œç„¶åè¾“å…¥åˆ°åŒå‘GRUç½‘ç»œä¸­ï¼Œç»“åˆä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè·å¾—è¯¥characterå¯¹åº”çš„éšè—çŠ¶æ€è¾“å‡º$h_{it}=[\overrightarrow{h_{it}}, \overleftarrow{h_{it}}]$
$$
x_{it} = W_cw_{it}, t \in [i,T]
$$

$$
\overrightarrow{h_{it}} = \overrightarrow{GRU}(x_{it})
$$

$$
\overleftarrow{h_{it}} = \overleftarrow{GRU}(x_{it})
$$

##### 2ã€word attention layer

attentionæœºåˆ¶çš„ç›®çš„å°±æ˜¯è¦æŠŠä¸€ä¸ªsegmentä¸­ï¼Œå¯¹segmentè¡¨è¾¾æœ€é‡è¦çš„characteræ‰¾å‡ºæ¥ï¼Œèµ‹äºˆä¸€ä¸ªæ›´å¤§çš„æ¯”é‡ã€‚

é¦–å…ˆå°†word encoderé‚£ä¸€æ­¥çš„è¾“å‡ºå¾—åˆ°çš„$h_{it}$è¾“å…¥åˆ°ä¸€ä¸ªå•å±‚çš„æ„ŸçŸ¥æœºä¸­å¾—åˆ°ç»“æœ$u_{it}$ä½œä¸ºå…¶éšå«è¡¨ç¤º
$$
u_{it} = tanh(W_wh_{it}+b_w)
$$
æ¥ç€ä¸ºäº†è¡¡é‡characterçš„é‡è¦æ€§ï¼Œå®šä¹‰äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„characterå±‚é¢ä¸Šä¸‹æ–‡å‘é‡$u_w$ï¼Œè®¡ç®—å…¶ä¸segmentä¸­æ¯ä¸ªcharaterçš„ç›¸ä¼¼åº¦ï¼Œç„¶åç»è¿‡ä¸€ä¸ª`softmax`æ“ä½œè·å¾—äº†ä¸€ä¸ªå½’ä¸€åŒ–çš„attentionæƒé‡çŸ©é˜µ$\alpha_{it}$ï¼Œä»£è¡¨segement iä¸­ç¬¬tä¸ªcharaterçš„æƒé‡ï¼š

$$
\alpha_{it} = \frac {exp(u_{it}^Tu_w)}{\sum_t(exp(u_{it}^Tu_w))}
$$
äºæ˜¯ï¼Œsegmentçš„å‘é‡$s_i$å°±å¯ä»¥çœ‹åšæ˜¯segmentä¸­characterçš„å‘é‡çš„åŠ æƒæ±‚å’Œã€‚è¿™é‡Œçš„characterå±‚é¢ä¸Šä¸‹æ–‡å‘é‡æ˜¯$u_w$éšæœºåˆå§‹åŒ–å¹¶ä¸”å¯ä»¥åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚
$$
s_i = \sum_{t} \alpha_{it}h_{it}
$$

##### 3ã€sentence encoder

é€šè¿‡ä¸Šè¿°æ­¥éª¤æˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªsegmentçš„å‘é‡è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨ç›¸ä¼¼çš„æ–¹æ³•å¾—åˆ°datagramå‘é‡$h_{i}=[\overrightarrow{h_{i}}, \overleftarrow{h_{i}}]$ï¼š
$$
\overleftarrow{h_{i}} = \overleftarrow{GRU}(s_{i}), i \in [i,L]
$$

$$
\overrightarrow{h_{i}} = \overrightarrow{GRU}(s_{i}), i \in [1,L]
$$

**4ã€sentence attention**

å’Œcharacterçº§åˆ«çš„attentionç±»ä¼¼ï¼Œä½¿ç”¨ä¸€ä¸ªsegmentçº§åˆ«çš„ä¸Šä¸‹æ–‡å‘é‡$u_s$,æ¥è¡¡é‡ä¸€ä¸ªsegmentåœ¨datagramçš„é‡è¦æ€§ã€‚
$$
u_{i} = tanh(W_sh_{i}+b_s)
$$

$$
\alpha_i = \frac {exp(u_i^Tu_s)}{\sum_t(exp(u_t^Tu_s))}
$$

$$
d = \sum_{t} \alpha_{i}h_{i}
$$

##### 5ã€softmax

ä¸Šé¢çš„$d$å‘é‡å°±æ˜¯æˆ‘ä»¬çš„åˆ°çš„æœ€åçš„datagramè¡¨ç¤ºï¼Œç„¶åè¾“å…¥ä¸€ä¸ªå…¨è¿æ¥çš„`softmax`å±‚è¿›è¡Œåˆ†ç±»å°±okäº†ã€‚

BSNN.pyä¸­åŒ…å«äº†æœ€é‡è¦çš„æ¨¡å‹ä»£ç :

```python
import tensorflow as tf
from tensorflow.contrib import rnn
from tensorflow.contrib import layers

def getSequenceLength(sequences):
    """
    :param sequences: æ‰€æœ‰çš„segmetné•¿åº¦ï¼Œ[a_size,b_size,c_size,,,]
    :return:æ¯ä¸ªsegmentè¿›è¡Œpaddingå‰çš„å®é™…å¤§å°
    """
    abs_sequences = tf.abs(sequences)
    # after padding data, max is 0
    abs_max_seq = tf.reduce_max(abs_sequences, reduction_indices=2)
    max_seq_sign = tf.sign(abs_max_seq)

    # sum is the real length
    real_len = tf.reduce_sum(max_seq_sign, reduction_indices=1)

    return tf.cast(real_len, tf.int32)

class BSNN(object):
    def __init__(self, max_sentence_num, max_sentence_length, num_classes, vocab_size,
                 embedding_size, learning_rate, decay_steps, decay_rate,
                 hidden_size, l2_lambda, grad_clip, is_training=False,
                 initializer=tf.random_normal_initializer(stddev=0.1)):
        
        self.vocab_size = vocab_size # å¯¹åº”æˆ‘çš„charæ•°é‡ 256
        self.max_sentence_num = max_sentence_num 
        self.max_sentence_length = max_sentence_length
        self.num_classes = num_classes # åˆ†ç±»æ•° 3
        self.embedding_size = embedding_size # å¯¹äºcharçš„emb_size=256 è®ºæ–‡ä¸­åˆå§‹åŒ–ä¸ºone-hot
        self.hidden_size = hidden_size 
        self.learning_rate = learning_rate
        self.decay_rate = decay_rate
        self.decay_steps = decay_steps
        self.l2_lambda = l2_lambda # l2èŒƒæ•°
        self.grad_clip = grad_clip
        self.initializer = initializer

        self.global_step = tf.Variable(0, trainable=False, name='global_step')

        # placeholder
        self.input_x = tf.placeholder(tf.int32, [None, max_sentence_num, max_sentence_length], name='input_x')
        self.input_y = tf.placeholder(tf.int32, [None, num_classes], name='input_y')
        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')

        if not is_training:
            return

        word_embedding = self.word2vec() # å¯¹äºcharè¡¨è¾¾ï¼Œåˆå§‹åŒ–ä¸ºone-hot
        sen_vec = self.sen2vec(word_embedding) # å¯¹åº”ä¸€ä¸ªsegmentè¡¨è¾¾
        doc_vec = self.doc2vec(sen_vec) # å¯¹åº”datagramè¡¨è¾¾

        self.logits = self.inference(doc_vec)
        self.loss_val = self.loss(self.input_y, self.logits)
        self.train_op = self.train()
        self.prediction = tf.argmax(self.logits, axis=1, name='prediction')
        self.pred_min = tf.reduce_min(self.prediction)
        self.pred_max = tf.reduce_max(self.prediction)
        self.pred_cnt = tf.bincount(tf.cast(self.prediction, dtype=tf.int32))
        self.label_cnt = tf.bincount(tf.cast(tf.argmax(self.input_y, axis=1), dtype=tf.int32))
        self.accuracy = self.accuracy(self.logits, self.input_y)

    def word2vec(self):
        with tf.name_scope('embedding'):
            self.embedding_mat = tf.Variable(tf.eye(self.vocab_size), name='embedding') # æ„é€ 256ç»´åº¦å•ä½one-hotå‘é‡
            # [batch, sen_in_doc, wrd_in_sent, embedding_size]
            word_embedding = tf.nn.embedding_lookup(self.embedding_mat, self.input_x)
            return word_embedding

    def BidirectionalGRUEncoder(self, inputs, name):
        """
        åŒå‘GRUç¼–ç å±‚ï¼Œå°†ä¸€segmentä¸­çš„æ‰€æœ‰characteræˆ–è€…ä¸€ä¸ªdatagramä¸­çš„æ‰€æœ‰segmentè¿›è¡Œç¼–ç å¾—åˆ°ä¸€ä¸ª2xhidden_sizeçš„è¾“å‡ºå‘é‡
        ç„¶ååœ¨è¾“å…¥inputsçš„shapeæ˜¯ï¼š
        input:[batch, max_time, embedding_size]
        output:[batch, max_time, 2*hidden_size]
        :return:
        """
        with tf.name_scope(name), tf.variable_scope(name, reuse=tf.AUTO_REUSE):  
            fw_gru_cell = rnn.GRUCell(self.hidden_size)
            bw_gru_cell = rnn.GRUCell(self.hidden_size)
            fw_gru_cell = rnn.DropoutWrapper(fw_gru_cell, output_keep_prob=self.dropout_keep_prob)
            bw_gru_cell = rnn.DropoutWrapper(bw_gru_cell, output_keep_prob=self.dropout_keep_prob)

            # fw_outputså’Œbw_outputsçš„sizeéƒ½æ˜¯[batch_size, max_time, hidden_size]
            (fw_outputs, bw_outputs), (fw_outputs_state, bw_outputs_state) = tf.nn.bidirectional_dynamic_rnn(
                cell_fw=fw_gru_cell, cell_bw=bw_gru_cell, inputs=inputs,
                sequence_length=getSequenceLength(inputs), dtype=tf.float32
            )
            # outputsçš„shapeæ˜¯[batch_size, max_time, hidden_size*2]
            outputs = tf.concat((fw_outputs, bw_outputs), 2)
            return outputs

    def AttentionLayer(self, inputs, name):
        """
        inputsæ˜¯GRUå±‚çš„è¾“å‡º
        inputs: [batch, max_time, 2*hidden_size]
        :return:
        """
        with tf.name_scope(name):
            context_weight = tf.Variable(tf.truncated_normal([self.hidden_size*2]), name='context_weight')

            # ä½¿ç”¨å•å±‚MLPå¯¹GRUçš„è¾“å‡ºè¿›è¡Œç¼–ç ï¼Œå¾—åˆ°éšè—å±‚è¡¨ç¤º
            # uit =tanh(Wwhit + bw)
            fc = layers.fully_connected(inputs, self.hidden_size*2, activation_fn=tf.nn.tanh)

            multiply = tf.multiply(fc, context_weight)
            reduce_sum = tf.reduce_sum(multiply, axis=2, keep_dims=True)
            # shape: [batch_size, max_time, 1]
            alpha = tf.nn.softmax(reduce_sum, dim=1)

            # shape: [batch_size, hidden_size*2]
            atten_output = tf.reduce_sum(tf.multiply(inputs, alpha), axis=1)
            return atten_output

    def sen2vec(self, word_embeded):
        with tf.name_scope('sen2vec'):
            """
            GRUçš„è¾“å…¥tensoræ˜¯[batch_size, max_time,...]ï¼Œåœ¨æ„é€ segmentå‘é‡æ—¶max_timeåº”è¯¥æ˜¯æ¯ä¸ªsegmentçš„é•¿åº¦ï¼Œ
            æ‰€ä»¥è¿™é‡Œå°†batch_size*sen_in_docå½“åšæ˜¯batch_sizeï¼Œè¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªGRUçš„cellå¤„ç†çš„éƒ½æ˜¯ä¸€ä¸ªcharacterçš„å‘é‡
            å¹¶æœ€ç»ˆå°†ä¸€segmentä¸­çš„æ‰€æœ‰characterçš„å‘é‡èåˆï¼ˆAttentionï¼‰åœ¨ä¸€èµ·å½¢æˆsegmentå‘é‡
            """
            # shapeï¼š[batch_size * sen_in_doc, word_in_sent, embedding_size]
            word_embeded = tf.reshape(word_embeded, [-1, self.max_sentence_length, self.embedding_size])

            # shape: [batch_size * sen_in_doc, word_in_sent, hiddeng_size * 2]
            word_encoder = self.BidirectionalGRUEncoder(word_embeded, name='word_encoder')

            # shape: [batch_size * sen_in_doc, hidden_size * 2]
            sen_vec = self.AttentionLayer(word_encoder, name='word_attention')
            return sen_vec

    def doc2vec(self, sen_vec):
        with tf.name_scope('doc2vec'):
            """
            è·Ÿsen2vecç±»ä¼¼ï¼Œä¸è¿‡è¿™é‡Œæ¯ä¸ªcellå¤„ç†çš„æ˜¯ä¸€ä¸ªsegmentçš„å‘é‡ï¼Œæœ€åèåˆæˆä¸ºdatagramçš„å‘é‡
            """
            sen_vec = tf.reshape(sen_vec, [-1, self.max_sentence_num, self.hidden_size*2])
            # shape: [batch_sizeï¼Œsen_in_doc, hidden_size * 2]
            doc_encoder = self.BidirectionalGRUEncoder(sen_vec, name='doc_encoder')
            # shape: [batch_sizeï¼Œhidden_size * 2]
            doc_vec = self.AttentionLayer(doc_encoder, name='doc_vec')
            return doc_vec

    def inference(self, doc_vec):
        with tf.name_scope('logits'):
            fc_out = layers.fully_connected(doc_vec, self.num_classes)
            return fc_out

    def accuracy(self, logits, input_y):
        with tf.name_scope('accuracy'):
            predict = tf.argmax(logits, axis=1, name='predict')
            label = tf.argmax(input_y, axis=1, name='label')
            acc = tf.reduce_mean(tf.cast(tf.equal(predict, label), tf.float32))
            return acc

    def loss(self, input_y, logits):
        with tf.name_scope('loss'):
            losses = tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits)
            loss = tf.reduce_mean(losses)
            if self.l2_lambda >0:
                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) for cand_var in tf.trainable_variables() if 'bia' not in cand_var.name])
                loss += self.l2_lambda * l2_loss
            return loss

    def train(self):
        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,
                                                   self.decay_steps, self.decay_rate, staircase=True)
        # use grad_clip to hand exploding or vanishing gradients
        optimizer = tf.train.AdamOptimizer(learning_rate)
        grads_and_vars = optimizer.compute_gradients(self.loss_val)
        for idx, (grad, var) in enumerate(grads_and_vars):
            if grad is not None:
                grads_and_vars[idx] = (tf.clip_by_norm(grad, self.grad_clip), var)

        train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)
        return train_op
```

#### è®­ç»ƒå‚æ•°

è®­ç»ƒå‚æ•°çš„è®¾å®šå¯ä»¥çœ‹ä¸‹é¢ä»£ç ä¸­configurationçš„å†…å®¹ï¼Œè¿™é‡Œè¯´ä¸€ä¸‹è®ºæ–‡ä¸­æåˆ°çš„å‡ ä¸ªå‚æ•°ï¼Œ$\gamma = 1$ï¼Œ$\alpha$å‡ç­‰ï¼Œsegmenté•¿åº¦è®¾å®šä¸º8ï¼Œbatch_sizeä¸º30ï¼ˆä¸åŸæ–‡ä¸€è‡´ï¼‰ï¼Œepoch=10ã€‚

train.py

```python
import tensorflow as tf
import time
import os
import datetime
from BSNN import BSNN
from DataUtil import read_dataset

#configuration
tf.flags.DEFINE_float("learning_rate",0.001,"learning rate")
tf.flags.DEFINE_integer("num_epochs",10,"embedding size")
tf.flags.DEFINE_integer("batch_size", 30, "Batch size for training/evaluating.") #æ‰¹å¤„ç†çš„å¤§å° 32-->128
tf.flags.DEFINE_integer("num_classes", 3, "number of classes")
tf.flags.DEFINE_integer("vocab_size", 256, "vocabulary size")
tf.flags.DEFINE_integer("decay_steps", 12000, "how many steps before decay learning rate.")
tf.flags.DEFINE_float("decay_rate", 0.9, "Rate of decay for learning rate.") #0.5ä¸€æ¬¡è¡°å‡å¤šå°‘

tf.flags.DEFINE_string("ckpt_dir","text_han_checkpoint/","checkpoint location for the model")
tf.flags.DEFINE_integer('num_checkpoints',5,'save checkpoints count')  

tf.flags.DEFINE_integer('max_sentence_num',100,'max sentence num in a doc')
tf.flags.DEFINE_integer('max_sentence_length',8,'max word count in a sentence')
tf.flags.DEFINE_integer("embedding_size",256,"embedding size")
tf.flags.DEFINE_integer('hidden_size',50,'cell output size')

tf.flags.DEFINE_boolean("is_training",True,"is traning.true:tranining,false:testing/inference")

tf.flags.DEFINE_integer("validate_every", 100, "Validate every validate_every epochs.") #æ¯10è½®åšä¸€æ¬¡éªŒè¯
tf.flags.DEFINE_float('validation_percentage',0.1,'validat data percentage in train data')

tf.flags.DEFINE_float("dropout_keep_prob", 0.5, "Dropout keep probability (default: 0.5)")

tf.flags.DEFINE_float("l2_reg_lambda", 0.0001, "L2 regularization lambda (default: 0.0)")
tf.flags.DEFINE_float('grad_clip',2.0,'grad_clip') # å’Œç±»åˆ«æ•°ç›¸å…³

tf.flags.DEFINE_boolean("allow_soft_placement", True, "Allow device soft device placement")
tf.flags.DEFINE_boolean("log_device_placement", False, "Log placement of ops on devices")


FLAGS = tf.flags.FLAGS

# load dataset
train_x, train_y, dev_x, dev_y = read_dataset()
print ("data load finished")

with tf.Graph().as_default():
    sess_conf = tf.ConfigProto(
        allow_soft_placement=FLAGS.allow_soft_placement,
        log_device_placement=FLAGS.log_device_placement)
    sess = tf.Session(config=sess_conf)

    with sess.as_default():
        timestamp = str(int(time.time()))
        out_dir = os.path.abspath(os.path.join(os.path.curdir, 'run_BSNN', timestamp))

        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

        checkpoint_dir = os.path.abspath(os.path.join(out_dir, FLAGS.ckpt_dir))
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        checkpoint_prefix = os.path.join(checkpoint_dir, 'model')

        bsnn = BSNN(FLAGS.max_sentence_num, FLAGS.max_sentence_length, FLAGS.num_classes, FLAGS.vocab_size, FLAGS.embedding_size,
                  FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.hidden_size, FLAGS.l2_reg_lambda,
                  FLAGS.grad_clip, FLAGS.is_training)
        
        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)
        sess.run(tf.global_variables_initializer())

        def train_step(x_batch, y_batch):
            feed_dict = {bsnn.input_x: x_batch,
                         bsnn.input_y: y_batch,
                         bsnn.dropout_keep_prob: FLAGS.dropout_keep_prob
                         }
            tmp, step, loss, accuracy, label_cnt, pred_cnt, pred_min, pred_max = sess.run(
                [bsnn.train_op, bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,
                 bsnn.pred_min, bsnn.pred_max,], feed_dict=feed_dict)

            print('train_label_cnt: ', label_cnt)
            print('train_min_max:', pred_min, pred_max)
            print('train_cnt: ', pred_cnt)
            time_str = datetime.datetime.now().isoformat()
            print("{}:step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))
            return step

        def dev_step(dev_x, dev_y):
            feed_dict = {bsnn.input_x: dev_x,
                         bsnn.input_y: dev_y,
                         bsnn.dropout_keep_prob: 1.0
                         }
            step, loss, accuracy, label_cnt, pred_cnt = sess.run(
                [bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,], feed_dict=feed_dict)

            time_str = datetime.datetime.now().isoformat()
            print("dev result: {}:step {}, loss {:g}, acc {:g}".format(time_str, step, loss, accuracy))


        for epoch in range(FLAGS.num_epochs):
            print('current epoch %s' % (epoch + 1))
            for i in range(0, 270, FLAGS.batch_size):
                x = train_x[i:i + FLAGS.batch_size]
                y = train_y[i:i + FLAGS.batch_size]
                train_step(x, y)
                cur_step = tf.train.global_step(sess, bsnn.global_step)

            print('\n')
            dev_step(dev_x, dev_y)
            path = saver.save(sess, checkpoint_prefix, global_step=epoch)
            print('Saved model checkpoint to {}\n'.format(path))
```

#### ç»“æœ

| epoch | ç»“æœ                        |
| ----- | --------------------------- |
| 1     | loss 1.17914, acc 0.206897  |
| 2     | loss 1.16526, acc 0.206897  |
| 3     | loss 1.17013, acc 0.206897  |
| 4     | loss 1.16233, acc 0.206897  |
| 5     | loss 1.14642, acc 0.448276  |
| 6     | loss 1.04893, acc 0.448276  |
| 7     | loss 0.898174, acc 0.448276 |
| 8     | loss 0.708184, acc 0.827586 |
| 9     | loss 0.616511, acc 0.827586 |
| 10    | loss 0.4711, acc 0.827586   |

éƒ¨åˆ†è¾“å‡ºç»†èŠ‚è§ä¸‹ï¼š

```
current epoch 1
2021-02-05T23:28:26.090626:step 1, loss 1.18368, acc 0.233333
2021-02-05T23:28:26.623201:step 2, loss 1.17141, acc 0.366667
2021-02-05T23:28:27.112586:step 3, loss 1.17092, acc 0.2
2021-02-05T23:28:27.600048:step 4, loss 1.16999, acc 0.333333
2021-02-05T23:28:28.092669:step 5, loss 1.16888, acc 0.4
2021-02-05T23:28:28.593807:step 6, loss 1.1664, acc 0.4
2021-02-05T23:28:29.085813:step 7, loss 1.17391, acc 0.233333
2021-02-05T23:28:29.577284:step 8, loss 1.16082, acc 0.433333
2021-02-05T23:28:30.078963:step 9, loss 1.17725, acc 0.3


dev result: 2021-02-05T23:28:30.484341:step 9, loss 1.17914, acc 0.206897
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-0

current epoch 2
2021-02-05T23:28:31.313493:step 10, loss 1.14458, acc 0.466667
2021-02-05T23:28:31.801910:step 11, loss 1.16036, acc 0.366667
2021-02-05T23:28:32.340277:step 12, loss 1.20258, acc 0.2
2021-02-05T23:28:32.840851:step 13, loss 1.16389, acc 0.333333
2021-02-05T23:28:33.345806:step 14, loss 1.14916, acc 0.4
2021-02-05T23:28:33.865011:step 15, loss 1.15131, acc 0.4
2021-02-05T23:28:34.375607:step 16, loss 1.16807, acc 0.233333
2021-02-05T23:28:34.888499:step 17, loss 1.15018, acc 0.433333
2021-02-05T23:28:35.416810:step 18, loss 1.16337, acc 0.3


dev result: 2021-02-05T23:28:35.609295:step 18, loss 1.16526, acc 0.206897
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-1

current epoch 3
2021-02-05T23:28:36.307796:step 19, loss 1.14345, acc 0.466667
2021-02-05T23:28:36.807924:step 20, loss 1.15106, acc 0.366667
2021-02-05T23:28:37.413876:step 21, loss 1.18131, acc 0.2
2021-02-05T23:28:37.979364:step 22, loss 1.1481, acc 0.333333
2021-02-05T23:28:38.528895:step 23, loss 1.13485, acc 0.4
2021-02-05T23:28:39.053765:step 24, loss 1.14369, acc 0.4
2021-02-05T23:28:39.566839:step 25, loss 1.16878, acc 0.233333
2021-02-05T23:28:40.087222:step 26, loss 1.1341, acc 0.433333
2021-02-05T23:28:40.603841:step 27, loss 1.1617, acc 0.3


dev result: 2021-02-05T23:28:40.796326:step 27, loss 1.17013, acc 0.206897
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-2

current epoch 4
2021-02-05T23:28:41.602442:step 28, loss 1.1145, acc 0.466667
2021-02-05T23:28:42.144761:step 29, loss 1.14097, acc 0.366667
2021-02-05T23:28:42.693294:step 30, loss 1.20376, acc 0.2
2021-02-05T23:28:43.229859:step 31, loss 1.14151, acc 0.333333
2021-02-05T23:28:43.755452:step 32, loss 1.12037, acc 0.4
2021-02-05T23:28:44.263125:step 33, loss 1.13507, acc 0.4
2021-02-05T23:28:44.771254:step 34, loss 1.16051, acc 0.233333
2021-02-05T23:28:45.294753:step 35, loss 1.11591, acc 0.433333
2021-02-05T23:28:45.826330:step 36, loss 1.14527, acc 0.3


dev result: 2021-02-05T23:28:46.004852:step 36, loss 1.16233, acc 0.206897
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-3

current epoch 5
2021-02-05T23:28:46.711379:step 37, loss 1.10953, acc 0.466667
2021-02-05T23:28:47.210262:step 38, loss 1.13091, acc 0.366667
2021-02-05T23:28:47.773246:step 39, loss 1.18087, acc 0.2
2021-02-05T23:28:48.286123:step 40, loss 1.13255, acc 0.333333
2021-02-05T23:28:48.789008:step 41, loss 1.11187, acc 0.4
2021-02-05T23:28:49.302116:step 42, loss 1.13908, acc 0.4
2021-02-05T23:28:49.805403:step 43, loss 1.1431, acc 0.266667
2021-02-05T23:28:50.314138:step 44, loss 1.11676, acc 0.433333
2021-02-05T23:28:50.827770:step 45, loss 1.12081, acc 0.366667


dev result: 2021-02-05T23:28:51.000757:step 45, loss 1.14642, acc 0.448276
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-4

current epoch 6
2021-02-05T23:28:51.710229:step 46, loss 1.1051, acc 0.566667
2021-02-05T23:28:52.197731:step 47, loss 1.09202, acc 0.5
2021-02-05T23:28:52.747259:step 48, loss 1.15978, acc 0.333333
2021-02-05T23:28:53.266820:step 49, loss 1.09888, acc 0.5
2021-02-05T23:28:53.778483:step 50, loss 1.04828, acc 0.6
2021-02-05T23:28:54.300956:step 51, loss 1.10639, acc 0.5
2021-02-05T23:28:54.807825:step 52, loss 1.06262, acc 0.5
2021-02-05T23:28:55.310532:step 53, loss 1.03075, acc 0.5
2021-02-05T23:28:55.833391:step 54, loss 1.02559, acc 0.466667


dev result: 2021-02-05T23:28:56.026642:step 54, loss 1.04893, acc 0.448276
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-5

current epoch 7
2021-02-05T23:28:56.747713:step 55, loss 0.996561, acc 0.566667
2021-02-05T23:28:57.233785:step 56, loss 0.973121, acc 0.5
2021-02-05T23:28:57.788470:step 57, loss 1.10553, acc 0.333333
2021-02-05T23:28:58.301127:step 58, loss 0.873215, acc 0.5
2021-02-05T23:28:58.816781:step 59, loss 0.839209, acc 0.633333
2021-02-05T23:28:59.331319:step 60, loss 0.928375, acc 0.533333
2021-02-05T23:28:59.847088:step 61, loss 0.837925, acc 0.5
2021-02-05T23:29:00.344677:step 62, loss 0.871867, acc 0.533333
2021-02-05T23:29:00.813062:step 63, loss 0.89659, acc 0.5


dev result: 2021-02-05T23:29:00.985665:step 63, loss 0.898174, acc 0.448276
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-6

current epoch 8
2021-02-05T23:29:01.699028:step 64, loss 0.867806, acc 0.633333
2021-02-05T23:29:02.186859:step 65, loss 0.826654, acc 0.633333
2021-02-05T23:29:02.737684:step 66, loss 1.01797, acc 0.433333
2021-02-05T23:29:03.243936:step 67, loss 0.765, acc 0.666667
2021-02-05T23:29:03.746920:step 68, loss 0.721655, acc 0.8
2021-02-05T23:29:04.245246:step 69, loss 0.937242, acc 0.8
2021-02-05T23:29:04.742633:step 70, loss 0.662761, acc 0.766667
2021-02-05T23:29:05.246319:step 71, loss 0.838433, acc 0.8
2021-02-05T23:29:05.758371:step 72, loss 0.69352, acc 0.833333


dev result: 2021-02-05T23:29:05.941858:step 72, loss 0.708184, acc 0.827586
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-7

current epoch 9
2021-02-05T23:29:06.681740:step 73, loss 0.615287, acc 0.833333
2021-02-05T23:29:07.155841:step 74, loss 0.634121, acc 0.833333
2021-02-05T23:29:07.706681:step 75, loss 0.863259, acc 0.7
2021-02-05T23:29:08.233564:step 76, loss 0.527865, acc 0.866667
2021-02-05T23:29:08.728556:step 77, loss 0.626868, acc 0.8
2021-02-05T23:29:09.239092:step 78, loss 0.655192, acc 0.833333
2021-02-05T23:29:09.747197:step 79, loss 0.518165, acc 0.833333
2021-02-05T23:29:10.261518:step 80, loss 0.557925, acc 0.866667
2021-02-05T23:29:10.763176:step 81, loss 0.600361, acc 0.833333


dev result: 2021-02-05T23:29:10.931726:step 81, loss 0.616511, acc 0.827586
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-8

current epoch 10
2021-02-05T23:29:11.619203:step 82, loss 0.579097, acc 0.833333
2021-02-05T23:29:12.099981:step 83, loss 0.531345, acc 0.833333
2021-02-05T23:29:12.636098:step 84, loss 0.777414, acc 0.7
2021-02-05T23:29:13.135171:step 85, loss 0.438025, acc 0.866667
2021-02-05T23:29:13.644192:step 86, loss 0.477813, acc 0.833333
2021-02-05T23:29:14.140262:step 87, loss 0.545707, acc 0.833333
2021-02-05T23:29:14.631534:step 88, loss 0.440086, acc 0.833333
2021-02-05T23:29:15.129455:step 89, loss 0.427449, acc 0.866667
2021-02-05T23:29:15.646563:step 90, loss 0.474299, acc 0.833333


dev result: 2021-02-05T23:29:15.826884:step 90, loss 0.4711, acc 0.827586
Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-9
```

#### ä¸è¶³

- æ•°æ®é›†æ”¶é›†æ—¶é—´æ¯”è¾ƒä»“ä¿ƒï¼Œå› æ­¤æ ·æœ¬ä¸å¤Ÿæ»¡æ„ï¼Œæ•°æ®é‡ç”±äºç”µè„‘æ€§èƒ½é™åˆ¶ä¹Ÿæ¯”è¾ƒå°‘
- Focal Lossé’ˆå¯¹æˆ‘çš„æ•°æ®ç‰¹ç‚¹ä½¿ç”¨$\alpha=[[1],[1],[1]],\gamma=1$ç­‰åŒäº`softmax_cross_entropy_with_logits`ï¼Œæ•ˆæœä¸é”™ï¼Œä½†æ²¡æœ‰åšè¯¦å°½çš„å¯¹æ¯”å®éªŒè¾ƒéš¾çœ‹å‡ºå®ƒçš„ä¼˜åŠ¿
- embeddingçš„ç»´åº¦è®¾ç½®çš„256ï¼Œä½†æ„Ÿè§‰æ•°æ®è¡¨ç¤ºæ¯”è¾ƒç¨€ç–
- å¯¹äºcharacterçš„è¿™ä¸€å±‚çš„è®­ç»ƒï¼Œæˆ‘çš„ç›´è§‰ä¸Šæ¥è¯´æ˜¯è¿œä¸å¦‚å•è¯è®­ç»ƒä¹‹äºå¥å­çš„æ„ä¹‰æ€§ï¼Œå› ä¸ºç›¸åŒçš„byteå¯èƒ½ä¸å…·æœ‰å¾ˆå¼ºçš„å…³è”æ€§ï¼Œå¦‚æœç›´æ¥å°†segmentçš„æ•°å­—è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå‡å°‘ä¸€å±‚ä¸çŸ¥é“æ•ˆæœæ˜¯å¦ä¼šé€€åŒ–å¾ˆå¤š

### å‚è€ƒé“¾æ¥

https://ieeexplore.ieee.org/abstract/document/8624128

https://www.aclweb.org/anthology/N16-1174.pdf

https://zihuaweng.github.io/2018/04/01/loss/

https://zhuanlan.zhihu.com/p/80594704

https://zhuanlan.zhihu.com/p/35571412